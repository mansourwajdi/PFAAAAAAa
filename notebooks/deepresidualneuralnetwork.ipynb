{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 3842332,
     "sourceType": "datasetVersion",
     "datasetId": 2286778
    },
    {
     "sourceId": 8052275,
     "sourceType": "datasetVersion",
     "datasetId": 4748749
    },
    {
     "sourceId": 8052287,
     "sourceType": "datasetVersion",
     "datasetId": 4748759
    },
    {
     "sourceId": 8052294,
     "sourceType": "datasetVersion",
     "datasetId": 4748764
    },
    {
     "sourceId": 27665,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 23306
    }
   ],
   "dockerImageVersionId": 30674,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#!pip install spafe"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-08T22:16:26.048930Z",
     "iopub.execute_input": "2024-04-08T22:16:26.049644Z",
     "iopub.status.idle": "2024-04-08T22:16:38.518039Z",
     "shell.execute_reply.started": "2024-04-08T22:16:26.049610Z",
     "shell.execute_reply": "2024-04-08T22:16:38.516853Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-09T16:49:47.573354500Z",
     "start_time": "2024-04-09T16:49:47.559756200Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "from DL.dataset import SoundFeatureDataset \n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from spafe.utils.preprocessing import pre_emphasis, framing, windowing, zero_handling\n",
    "from numpy import log, exp, infty, zeros_like, vstack, zeros, errstate, finfo, sqrt, floor, tile, concatenate, arange, \\\n",
    "    meshgrid, ceil, linspace\n",
    "from scipy.interpolate import interpn\n",
    "from scipy.special import logsumexp\n",
    "from scipy.signal import lfilter\n",
    "from scipy.fft import dct\n",
    "import librosa\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-08T22:16:38.520012Z",
     "iopub.execute_input": "2024-04-08T22:16:38.520337Z",
     "iopub.status.idle": "2024-04-08T22:16:38.527938Z",
     "shell.execute_reply.started": "2024-04-08T22:16:38.520305Z",
     "shell.execute_reply": "2024-04-08T22:16:38.526852Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T10:50:48.426102800Z",
     "start_time": "2024-04-15T10:50:31.576259400Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def envelope(y, rate, threshold):\n",
    "    mask = []\n",
    "    y = pd.Series(y).apply(np.abs)\n",
    "    y_mean = y.rolling(window=int(rate / 10), min_periods=1, center=True).mean()\n",
    "    for mean in y_mean:\n",
    "        if mean > threshold:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def clean_audio(file_path):\n",
    "    signal, rate = sf.read(file_path)\n",
    "    mask = envelope(signal, rate, 0.0005)\n",
    "    signal = signal[mask]\n",
    "    signal = signal[: rate]\n",
    "\n",
    "    return signal, rate\n",
    "\n",
    "\n",
    "def pad(x, max_len=64000):\n",
    "    x_len = x.shape[0]\n",
    "    if x_len >= max_len:\n",
    "        return x[:max_len]\n",
    "    num_repeats = (max_len / x_len) + 1\n",
    "    x_repeat = np.repeat(x, num_repeats)\n",
    "    padded_x = x_repeat[:max_len]\n",
    "    return padded_x\n",
    "def get_log_spectrum(x):\n",
    "    s = librosa.core.stft(x, n_fft=2048, win_length=2048, hop_length=512)\n",
    "    a = np.abs(s) ** 2\n",
    "    feat = librosa.power_to_db(a)\n",
    "    return feat\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-08T22:16:38.529049Z",
     "iopub.execute_input": "2024-04-08T22:16:38.529394Z",
     "iopub.status.idle": "2024-04-08T22:16:38.544828Z",
     "shell.execute_reply.started": "2024-04-08T22:16:38.529359Z",
     "shell.execute_reply": "2024-04-08T22:16:38.543960Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T10:50:48.440781Z",
     "start_time": "2024-04-15T10:50:48.429256100Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LFCC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from spafe.utils.exceptions import ErrorMsgs\n",
    "\n",
    "def linear_filter_banks(nfilts=20,\n",
    "                        nfft=512,\n",
    "                        fs=16000,\n",
    "                        low_freq=None,\n",
    "                        high_freq=None,\n",
    "                        scale=\"constant\"):\n",
    "    \"\"\"\n",
    "    Compute linear-filterbanks. The filters are stored in the rows, the columns\n",
    "    correspond to fft bins.\n",
    "\n",
    "    Args:\n",
    "        nfilts    (int) : the number of filters in the filterbank.\n",
    "                          (Default 20)\n",
    "        nfft      (int) : the FFT size.\n",
    "                          (Default is 512)\n",
    "        fs        (int) : sample rate/ sampling frequency of the signal.\n",
    "                          (Default 16000 Hz)\n",
    "        low_freq  (int) : lowest band edge of linear filters.\n",
    "                          (Default 0 Hz)\n",
    "        high_freq (int) : highest band edge of linear filters.\n",
    "                          (Default samplerate/2)\n",
    "        scale    (str)  : choose if max bins amplitudes ascend, descend or are constant (=1).\n",
    "                          Default is \"constant\"\n",
    "\n",
    "    Returns:\n",
    "        (numpy array) array of size nfilts * (nfft/2 + 1) containing filterbank.\n",
    "        Each row holds 1 filter.\n",
    "    \"\"\"\n",
    "    # init freqs\n",
    "    high_freq = high_freq or fs / 2\n",
    "    low_freq = low_freq or 0\n",
    "\n",
    "    # run checks\n",
    "\n",
    "\n",
    "    # compute points evenly spaced in frequency (points are in Hz)\n",
    "    linear_points = np.linspace(low_freq, high_freq, nfilts + 2)\n",
    "\n",
    "    # we use fft bins, so we have to convert from Hz to fft bin number\n",
    "    bins = np.floor((nfft + 1) * linear_points / fs)\n",
    "    fbank = np.zeros([nfilts, nfft // 2 + 1])\n",
    "\n",
    "    # init scaler\n",
    "    if scale == \"descendant\" or scale == \"constant\":\n",
    "        c = 1\n",
    "    else:\n",
    "        c = 0\n",
    "\n",
    "    # compute amps of fbanks\n",
    "    for j in range(0, nfilts):\n",
    "        b0, b1, b2 = bins[j], bins[j + 1], bins[j + 2]\n",
    "\n",
    "        # compute scaler\n",
    "        if scale == \"descendant\":\n",
    "            c -= 1 / nfilts\n",
    "            c = c * (c > 0) + 0 * (c < 0)\n",
    "\n",
    "        elif scale == \"ascendant\":\n",
    "            c += 1 / nfilts\n",
    "            c = c * (c < 1) + 1 * (c > 1)\n",
    "\n",
    "        # compute fbanks\n",
    "        fbank[j, int(b0):int(b1)] = c * (np.arange(int(b0), int(b1)) -\n",
    "                                         int(b0)) / (b1 - b0)\n",
    "        fbank[j, int(b1):int(b2)] = c * (\n",
    "            int(b2) - np.arange(int(b1), int(b2))) / (b2 - b1)\n",
    "\n",
    "    return np.abs(fbank)\n",
    "\n",
    "def lfcc(sig,\n",
    "         fs=16000,\n",
    "         num_ceps=20,\n",
    "         pre_emph=0,\n",
    "         pre_emph_coeff=0.97,\n",
    "         win_len=0.030,\n",
    "         win_hop=0.015,\n",
    "         win_type=\"hamming\",\n",
    "         nfilts=70,\n",
    "         nfft=1024,\n",
    "         low_freq=None,\n",
    "         high_freq=None,\n",
    "         scale=\"constant\",\n",
    "         dct_type=2,\n",
    "         normalize=0):\n",
    "    \"\"\"\n",
    "    Compute the linear-frequency cepstral coefficients (GFCC features) from an audio signal.\n",
    "    Args:\n",
    "        sig            (array) : a mono audio signal (Nx1) from which to compute features.\n",
    "        fs               (int) : the sampling frequency of the signal we are working with.\n",
    "                                 Default is 16000.\n",
    "        num_ceps       (float) : number of cepstra to return.\n",
    "                                 Default is 13.\n",
    "        pre_emph         (int) : apply pre-emphasis if 1.\n",
    "                                 Default is 1.\n",
    "        pre_emph_coeff (float) : apply pre-emphasis filter [1 -pre_emph] (0 = none).\n",
    "                                 Default is 0.97.\n",
    "        win_len        (float) : window length in sec.\n",
    "                                 Default is 0.025.\n",
    "        win_hop        (float) : step between successive windows in sec.\n",
    "                                 Default is 0.01.\n",
    "        win_type       (float) : window type to apply for the windowing.\n",
    "                                 Default is \"hamming\".\n",
    "        nfilts           (int) : the number of filters in the filterbank.\n",
    "                                 Default is 40.\n",
    "        nfft             (int) : number of FFT points.\n",
    "                                 Default is 512.\n",
    "        low_freq         (int) : lowest band edge of mel filters (Hz).\n",
    "                                 Default is 0.\n",
    "        high_freq        (int) : highest band edge of mel filters (Hz).\n",
    "                                 Default is samplerate / 2 = 8000.\n",
    "        scale           (str)  : choose if max bins amplitudes ascend, descend or are constant (=1).\n",
    "                                 Default is \"constant\".\n",
    "        dct_type         (int) : type of DCT used - 1 or 2 (or 3 for HTK or 4 for feac).\n",
    "                                 Default is 2.\n",
    "        use_energy       (int) : overwrite C0 with true log energy\n",
    "                                 Default is 0.\n",
    "        lifter           (int) : apply liftering if value > 0.\n",
    "                                 Default is 22.\n",
    "        normalize        (int) : apply normalization if 1.\n",
    "                                 Default is 0.\n",
    "    Returns:\n",
    "        (array) : 2d array of LFCC features (num_frames x num_ceps)\n",
    "    \"\"\"\n",
    "    # init freqs\n",
    "    high_freq = high_freq or fs / 2\n",
    "    low_freq = low_freq or 0\n",
    "\n",
    "    # run checks\n",
    "    \"\"\"\n",
    "    if low_freq < 0:\n",
    "        raise ParameterError(ErrorMsgs[\"low_freq\"])\n",
    "    if high_freq > (fs / 2):\n",
    "        raise ParameterError(ErrorMsgs[\"high_freq\"])\n",
    "    if nfilts < num_ceps:\n",
    "        raise ParameterError(ErrorMsgs[\"nfilts\"])\n",
    "\"\"\"\n",
    "    # pre-emphasis\n",
    "    if pre_emph:\n",
    "        sig = pre_emphasis(sig=sig, pre_emph_coeff=pre_emph_coeff)\n",
    "\n",
    "    # -> framing\n",
    "    frames, frame_length = framing(sig=sig,\n",
    "                                   fs=fs,\n",
    "                                   win_len=win_len,\n",
    "                                   win_hop=win_hop)\n",
    "\n",
    "    # -> windowing\n",
    "    windows = windowing(frames=frames,\n",
    "                        frame_len=frame_length,\n",
    "                        win_type=win_type)\n",
    "\n",
    "    # -> FFT -> |.|\n",
    "    fourrier_transform = np.fft.rfft(windows, nfft)\n",
    "    abs_fft_values = np.abs(fourrier_transform) ** 2\n",
    "\n",
    "    #  -> x linear-fbanks\n",
    "    linear_fbanks_mat = linear_filter_banks(nfilts=nfilts,\n",
    "                                            nfft=nfft,\n",
    "                                            fs=fs,\n",
    "                                            low_freq=low_freq,\n",
    "                                            high_freq=high_freq,\n",
    "                                            scale=scale)\n",
    "    features = np.dot(abs_fft_values, linear_fbanks_mat.T)\n",
    "\n",
    "    log_features = np.log10(features + 2.2204e-16)\n",
    "\n",
    "    #  -> DCT(.)\n",
    "    lfccs = dct(log_features, type=dct_type, norm='ortho', axis=1)[:, :num_ceps]\n",
    "\n",
    "    return lfccs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Deltas(x, width=3):\n",
    "    hlen = int(floor(width/2))\n",
    "    win = list(range(hlen, -hlen-1, -1))\n",
    "    xx_1 = tile(x[:, 0], (1, hlen)).reshape(hlen, -1).T\n",
    "    xx_2 = tile(x[:, -1], (1, hlen)).reshape(hlen, -1).T\n",
    "    xx = concatenate([xx_1, x, xx_2], axis=-1)\n",
    "    D = lfilter(win, 1, xx)\n",
    "    return D[:, hlen*2:]\n",
    "\n",
    "\n",
    "def extract_lfcc(sig,fs=16000, num_ceps=20, order_deltas=2, low_freq=0, high_freq=4000):\n",
    "    # put VAD here, if wanted\n",
    "    lfccs = lfcc(sig=sig,\n",
    "                 fs=fs,\n",
    "                 num_ceps=num_ceps,\n",
    "                 low_freq=low_freq,\n",
    "                 high_freq=high_freq).T\n",
    "    if order_deltas > 0:\n",
    "        feats = list()\n",
    "        feats.append(lfccs)\n",
    "        for d in range(order_deltas):\n",
    "            feats.append(Deltas(feats[-1]))\n",
    "        lfccs = vstack(feats)\n",
    "    return lfccs"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-08T22:16:38.546894Z",
     "iopub.execute_input": "2024-04-08T22:16:38.547161Z",
     "iopub.status.idle": "2024-04-08T22:16:38.574585Z",
     "shell.execute_reply.started": "2024-04-08T22:16:38.547138Z",
     "shell.execute_reply": "2024-04-08T22:16:38.573706Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T10:50:48.448818900Z",
     "start_time": "2024-04-15T10:50:48.443941300Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set = SoundFeatureDataset('../Data/PA','../DL/mfcc',is_logical=False, is_train=True)\n",
    "dev_set = SoundFeatureDataset('../Data/PA','../DL/mfcc',is_logical=False, is_train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T10:51:46.065708200Z",
     "start_time": "2024-04-15T10:51:45.995918900Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "# import torch\n",
    "# import collections\n",
    "# import os\n",
    "# import soundfile as sf\n",
    "# import librosa\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import numpy as np\n",
    "# from joblib import Parallel, delayed\n",
    "# import h5py\n",
    "# import random\n",
    "# \n",
    "# \n",
    "# ASVFile = collections.namedtuple('ASVFile',\n",
    "#                                  ['speaker_id', 'file_name', 'path', 'sys_id', 'key'])\n",
    "# \n",
    "# class ASVDataset(Dataset):\n",
    "#     def __init__(self,data_root,transform=None, is_train=True, sample_size=None,\n",
    "#                  is_logical=True, feature_name=None, is_eval=False,cache_name=None,random_sample=None,\n",
    "#                  eval_part=0):\n",
    "# \n",
    "# \n",
    "#         if is_logical:\n",
    "#             track = 'LA'\n",
    "#         else:\n",
    "# \n",
    "#             track = 'PA'\n",
    "#         assert feature_name is not None, 'must provide feature name'\n",
    "#         self.track = track\n",
    "#         self.is_logical = is_logical\n",
    "#         self.prefix = 'ASVspoof2019_{}'.format(track)\n",
    "#         v1_suffix = ''\n",
    "#         if is_eval and track == 'PA':\n",
    "#             v1_suffix = '_v1'\n",
    "# \n",
    "#         self.is_eval = is_eval\n",
    "#         self.data_root = data_root\n",
    "#         self.dset_name = 'eval' if is_eval else 'train' if is_train else 'dev'\n",
    "#         self.protocols_fname = 'eval.trl'.format(eval_part) if is_eval else 'train.trn' if is_train else 'dev.trl'\n",
    "#         self.protocols_dir = os.path.join(self.data_root,\n",
    "#                                           '{}_cm_protocols/'.format(self.prefix))\n",
    "#         self.files_dir = os.path.join(self.data_root, '{}_{}'.format(\n",
    "#             self.prefix, self.dset_name) + v1_suffix, 'flac')\n",
    "#         self.protocols_fname = os.path.join(self.protocols_dir,\n",
    "#                                             'ASVspoof2019.{}.cm.{}.txt'.format(track, self.protocols_fname))\n",
    "#         self.cache_fname = 'cache_{}{}_{}_{}.npy'.format(self.dset_name,\n",
    "#                                                          '_part{}'.format(eval_part) if is_eval else '', track,\n",
    "#                                                          feature_name)\n",
    "#         self.cache_matlab_fname = 'cache_{}{}_{}_{}.mat'.format(\n",
    "#             self.dset_name, '_part{}'.format(eval_part) if is_eval else '',\n",
    "#             track, feature_name)\n",
    "#         self.transform = transform\n",
    "#         self.random_length=random_sample\n",
    "#         print(transform)\n",
    "#         if os.path.exists(self.cache_fname):\n",
    "#             self.data_x, self.data_y,self.data_filename = torch.load(self.cache_fname)\n",
    "#             print('Dataset loaded from cache ', self.cache_fname)\n",
    "#         elif feature_name == 'cqcc':\n",
    "#             if os.path.exists(self.cache_matlab_fname):\n",
    "#                 self.data_x, self.data_y, self.data_filename = self.read_matlab_cache(self.cache_matlab_fname)\n",
    "#                 self.files_meta = self.parse_protocols_file(self.protocols_fname)\n",
    "#                 print('Dataset loaded from matlab cache ', self.cache_matlab_fname)\n",
    "#                 torch.save((self.data_x, self.data_y, self.data_filename, self.files_meta),\n",
    "#                            self.cache_fname, pickle_protocol=4)\n",
    "#                 print('Dataset saved to cache ', self.cache_fname)\n",
    "#             else:\n",
    "#                 print(\"Matlab cache for cqcc feature do not exist.\")\n",
    "#         else:\n",
    "#             self.files_meta = self.parse_protocols_file(self.protocols_fname)\n",
    "#             if self.random_length:\n",
    "#                 random_files_meta = random.sample(self.files_meta, min(len(self.files_meta), self.random_length))\n",
    "#                 data = list(map(self.read_file, random_files_meta))\n",
    "#             else:\n",
    "#                 data = list(map(self.read_file, self.files_meta))\n",
    "#             self.data_x, self.data_y, self.data_filename = map(list, zip(*data))\n",
    "#             if self.transform is not None:\n",
    "#                 # self.data_x = list(map(self.transform, self.data_x))\n",
    "#                 self.data_x = Parallel(n_jobs=4, prefer='threads')(delayed(self.transform)(x) for x in self.data_x)\n",
    "#             torch.save((self.data_x, self.data_y,self.data_filename), self.cache_fname)\n",
    "#             print('Dataset saved to cache ', self.cache_fname)\n",
    "#         if sample_size:\n",
    "#             select_idx = np.random.choice(len(self.files_meta), size=(sample_size,), replace=True).astype(np.int32)\n",
    "#             self.files_meta = [self.files_meta[x] for x in select_idx]\n",
    "#             self.data_x = [self.data_x[x] for x in select_idx]\n",
    "#             self.data_y = [self.data_y[x] for x in select_idx]\n",
    "#             self.data_sysid = [self.data_filename[x] for x in select_idx]\n",
    "#         self.length = len(self.data_x)\n",
    "# \n",
    "#     def __len__(self):\n",
    "#         return self.length\n",
    "# \n",
    "#     def __getitem__(self, idx):\n",
    "#         x = self.data_x[idx]\n",
    "#         y = self.data_y[idx]\n",
    "#         return x, y,self.data_filename[idx]\n",
    "# \n",
    "#     def read_file(self, meta):\n",
    "#         data_x, sample_rate = sf.read(meta.path)\n",
    "#         data_y = meta.key\n",
    "#         return data_x, float(data_y), meta.file_name\n",
    "# \n",
    "#     def _parse_line(self, line):\n",
    "#         tokens = line.strip().split(' ')\n",
    "#         return ASVFile(speaker_id=tokens[0],\n",
    "#                        file_name=tokens[1],\n",
    "#                        path=os.path.join(self.files_dir, tokens[1] + '.flac'),\n",
    "#                        sys_id=0,\n",
    "#                        key=int(tokens[4] == 'spoof'))\n",
    "# \n",
    "#     def parse_protocols_file(self, protocols_fname):\n",
    "#         lines = open(protocols_fname).readlines()\n",
    "#         files_meta = map(self._parse_line, lines)\n",
    "#         return list(files_meta)\n",
    "# \n",
    "#     def read_matlab_cache(self, filepath):\n",
    "#         f = h5py.File(filepath, 'r')\n",
    "#         # filename_index = f[\"filename\"]\n",
    "#         # filename = []\n",
    "#         data_x_index = f[\"data_x\"]\n",
    "#         sys_id_index = f[\"sys_id\"]\n",
    "#         data_x = []\n",
    "#         data_y = f[\"data_y\"][0]\n",
    "#         sys_id = []\n",
    "#         for i in range(0, data_x_index.shape[1]):\n",
    "#             idx = data_x_index[0][i]  # data_x\n",
    "#             temp = f[idx]\n",
    "#             data_x.append(np.array(temp).transpose())\n",
    "#             # idx = filename_index[0][i]  # filename\n",
    "#             # temp = list(f[idx])\n",
    "#             # temp_name = [chr(x[0]) for x in temp]\n",
    "#             # filename.append(''.join(temp_name))\n",
    "#             idx = sys_id_index[0][i]  # sys_id\n",
    "#             temp = f[idx]\n",
    "#             sys_id.append(int(list(temp)[0][0]))\n",
    "#         data_x = np.array(data_x)\n",
    "#         data_y = np.array(data_y)\n",
    "#         return data_x.astype(np.float32), data_y.astype(np.int64), sys_id"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-08T22:16:38.576018Z",
     "iopub.execute_input": "2024-04-08T22:16:38.576569Z",
     "iopub.status.idle": "2024-04-08T22:16:38.606549Z",
     "shell.execute_reply.started": "2024-04-08T22:16:38.576535Z",
     "shell.execute_reply": "2024-04-08T22:16:38.605658Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T20:36:56.882063100Z",
     "start_time": "2024-04-14T20:36:56.876432400Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOSS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd.function import Function\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class OCSoftmax(nn.Module):\n",
    "    def __init__(self, feat_dim=2, r_real=0.9, r_fake=0.5, alpha=20.0):\n",
    "        super(OCSoftmax, self).__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.r_real = r_real\n",
    "        self.r_fake = r_fake\n",
    "        self.alpha = alpha\n",
    "        self.center = nn.Parameter(torch.randn(1, self.feat_dim))\n",
    "        nn.init.kaiming_uniform_(self.center, 0.25)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        w = F.normalize(self.center, p=2, dim=1)\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "\n",
    "        scores = x @ w.transpose(0,1)\n",
    "        output_scores = scores.clone()\n",
    "\n",
    "        scores[labels == 0] = self.r_real - scores[labels == 0]\n",
    "        scores[labels == 1] = scores[labels == 1] - self.r_fake\n",
    "\n",
    "        loss = self.softplus(self.alpha * scores).mean()\n",
    "\n",
    "        return loss, output_scores.squeeze(1)\n",
    "\n",
    "class AMSoftmax(nn.Module):\n",
    "    def __init__(self, num_classes, enc_dim, s=20, m=0.9):\n",
    "        super(AMSoftmax, self).__init__()\n",
    "        self.enc_dim = enc_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.centers = nn.Parameter(torch.randn(num_classes, enc_dim))\n",
    "\n",
    "    def forward(self, feat, label):\n",
    "        batch_size = feat.shape[0]\n",
    "        norms = torch.norm(feat, p=2, dim=-1, keepdim=True)\n",
    "        nfeat = torch.div(feat, norms)\n",
    "\n",
    "        norms_c = torch.norm(self.centers, p=2, dim=-1, keepdim=True)\n",
    "        ncenters = torch.div(self.centers, norms_c)\n",
    "        logits = torch.matmul(nfeat, torch.transpose(ncenters, 0, 1))\n",
    "\n",
    "        y_onehot = torch.FloatTensor(batch_size, self.num_classes)\n",
    "        y_onehot.zero_()\n",
    "        y_onehot = Variable(y_onehot).cuda()\n",
    "        y_onehot.scatter_(1, torch.unsqueeze(label, dim=-1), self.m)\n",
    "        margin_logits = self.s * (logits - y_onehot)\n",
    "\n",
    "        return logits, margin_logits"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-08T22:16:38.607691Z",
     "iopub.execute_input": "2024-04-08T22:16:38.608006Z",
     "iopub.status.idle": "2024-04-08T22:16:38.624748Z",
     "shell.execute_reply.started": "2024-04-08T22:16:38.607976Z",
     "shell.execute_reply": "2024-04-08T22:16:38.623850Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T10:52:09.804159Z",
     "start_time": "2024-04-15T10:52:09.784616600Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, n_in, n_out, ks=1, ndim=1, norm_type=None, act_cls=None, bias=False):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv1d(n_in, n_out, kernel_size=ks, bias=bias)\n",
    "        self.norm = None\n",
    "        if norm_type:\n",
    "            if norm_type == NormType.Batch:\n",
    "                self.norm = nn.BatchNorm1d(n_out)\n",
    "            elif norm_type == NormType.Group:\n",
    "                self.norm = nn.GroupNorm(1, n_out)\n",
    "            elif norm_type == NormType.Layer:\n",
    "                self.norm = nn.LayerNorm(n_out)\n",
    "            elif norm_type == NormType.Spectral:\n",
    "                self.norm = nn.utils.spectral_norm\n",
    "        self.activation = act_cls() if act_cls else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"Self attention layer for n_channels.\"\n",
    "    def __init__(self, n_channels):\n",
    "        super(SelfAttention, self).__init__()  # Call the superclass's __init__() method\n",
    "        self.query, self.key, self.value = [self._conv(n_channels, c) for c in (n_channels//8, n_channels//8, n_channels)]\n",
    "        self.gamma = nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "    def _conv(self, n_in, n_out):\n",
    "        return ConvLayer(n_in, n_out, ks=1, ndim=1, norm_type=None, act_cls=None, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.size()\n",
    "        x = x.view(*size[:2], -1)\n",
    "        f, g, h = self.query(x), self.key(x), self.value(x)\n",
    "        beta = F.softmax(torch.bmm(f.transpose(1, 2), g), dim=1)\n",
    "        o = self.gamma * torch.bmm(h, beta) + x\n",
    "        return o.view(*size).contiguous()\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride, *args, **kwargs):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    '''Pre-activation version of the original Bottleneck module.'''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride, *args, **kwargs):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "RESNET_CONFIGS = {'18': [[2, 2, 2, 2], PreActBlock],\n",
    "                  '28': [[3, 4, 6, 3], PreActBlock],\n",
    "                  '34': [[3, 4, 6, 3], PreActBlock],\n",
    "                  '50': [[3, 4, 6, 3], PreActBottleneck],\n",
    "                  '101': [[3, 4, 23, 3], PreActBottleneck]\n",
    "                  }\n",
    "\n",
    "def setup_seed(random_seed, cudnn_deterministic=True):\n",
    "    # initialization\n",
    "    torch.manual_seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(random_seed)\n",
    "        torch.backends.cudnn.deterministic = cudnn_deterministic\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_nodes, enc_dim, resnet_type='18', nclasses=2):\n",
    "        self.in_planes = 16\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        layers, block = RESNET_CONFIGS[resnet_type]\n",
    "\n",
    "        self._norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(9, 3), stride=(3, 1), padding=(1, 1), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(512 * block.expansion, 256, kernel_size=(num_nodes, 3), stride=(1, 1), padding=(0, 1),\n",
    "                               bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.fl=nn.Flatten()\n",
    "        self.fc = nn.Linear(256 * 2, enc_dim)\n",
    "        self.fc_mu = nn.Linear(enc_dim, nclasses) if nclasses >= 2 else nn.Linear(enc_dim, 1)\n",
    "\n",
    "        self.initialize_params()\n",
    "        self.attention = SelfAttention(256)\n",
    "        self.fc1=nn.LazyLinear(512)\n",
    "\n",
    "    def initialize_params(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, torch.nn.Conv2d):\n",
    "                init.kaiming_normal_(layer.weight, a=0, mode='fan_out')\n",
    "            elif isinstance(layer, torch.nn.Linear):\n",
    "                init.kaiming_uniform_(layer.weight)\n",
    "            elif isinstance(layer, torch.nn.BatchNorm2d) or isinstance(layer, torch.nn.BatchNorm1d):\n",
    "                layer.weight.data.fill_(1)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride=1):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(conv1x1(self.in_planes, planes * block.expansion, stride),\n",
    "                                       norm_layer(planes * block.expansion))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_planes, planes, stride, downsample, 1, 64, 1, norm_layer))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(\n",
    "                block(self.in_planes, planes, 1, groups=1, base_width=64, dilation=False, norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(self.bn1(x))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.conv5(x)\n",
    "        x_shape=x.shape\n",
    "        x = self.activation(self.bn5(x)).view(x_shape[0],x_shape[1], -1)\n",
    "        stats = self.attention(x)\n",
    "        stats=self.fl(stats)\n",
    "        stats=self.fc1(stats)\n",
    "        feat = self.fc(stats)\n",
    "\n",
    "        mu = self.fc_mu(feat)\n",
    "    \n",
    "        return feat, mu\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T10:52:38.736945Z",
     "start_time": "2024-04-15T10:52:38.723875600Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#OLD VERSION\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, mean_only=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        #self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.att_weights = nn.Parameter(torch.Tensor(1, hidden_size),requires_grad=True)\n",
    "\n",
    "        self.mean_only = mean_only\n",
    "\n",
    "        init.kaiming_uniform_(self.att_weights)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        weights = torch.bmm(inputs, self.att_weights.permute(1, 0).unsqueeze(0).repeat(batch_size, 1, 1))\n",
    "\n",
    "        if inputs.size(0)==1:\n",
    "            attentions = F.softmax(torch.tanh(weights),dim=1)\n",
    "            weighted = torch.mul(inputs, attentions.expand_as(inputs))\n",
    "        else:\n",
    "            attentions = F.softmax(torch.tanh(weights.squeeze()),dim=1)\n",
    "            weighted = torch.mul(inputs, attentions.unsqueeze(2).expand_as(inputs))\n",
    "\n",
    "        if self.mean_only:\n",
    "            return weighted.sum(1)\n",
    "        else:\n",
    "            noise = 1e-5*torch.randn(weighted.size())\n",
    "\n",
    "            if inputs.is_cuda:\n",
    "                noise = noise.to(inputs.device)\n",
    "            avg_repr, std_repr = weighted.sum(1), (weighted+noise).std(1)\n",
    "\n",
    "            representations = torch.cat((avg_repr,std_repr),1)\n",
    "\n",
    "            return representations\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride, *args, **kwargs):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    '''Pre-activation version of the original Bottleneck module.'''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride, *args, **kwargs):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "RESNET_CONFIGS = {'18': [[2, 2, 2, 2], PreActBlock],\n",
    "                  '28': [[3, 4, 6, 3], PreActBlock],\n",
    "                  '34': [[3, 4, 6, 3], PreActBlock],\n",
    "                  '50': [[3, 4, 6, 3], PreActBottleneck],\n",
    "                  '101': [[3, 4, 23, 3], PreActBottleneck]\n",
    "                  }\n",
    "\n",
    "def setup_seed(random_seed, cudnn_deterministic=True):\n",
    "    # initialization\n",
    "    torch.manual_seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(random_seed)\n",
    "        torch.backends.cudnn.deterministic = cudnn_deterministic\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_nodes, enc_dim, resnet_type='18', nclasses=2):\n",
    "        self.in_planes = 16\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        layers, block = RESNET_CONFIGS[resnet_type]\n",
    "\n",
    "        self._norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(9, 3), stride=(3, 1), padding=(1, 1), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(512 * block.expansion, 256, kernel_size=(num_nodes, 3), stride=(1, 1), padding=(0, 1),\n",
    "                               bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.fc = nn.Linear(256 * 2, enc_dim)\n",
    "        self.fc_mu = nn.Linear(enc_dim, nclasses) if nclasses >= 2 else nn.Linear(enc_dim, 1)\n",
    "\n",
    "        self.initialize_params()\n",
    "        self.attention = SelfAttention(256)\n",
    "\n",
    "    def initialize_params(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, torch.nn.Conv2d):\n",
    "                init.kaiming_normal_(layer.weight, a=0, mode='fan_out')\n",
    "            elif isinstance(layer, torch.nn.Linear):\n",
    "                init.kaiming_uniform_(layer.weight)\n",
    "            elif isinstance(layer, torch.nn.BatchNorm2d) or isinstance(layer, torch.nn.BatchNorm1d):\n",
    "                layer.weight.data.fill_(1)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride=1):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(conv1x1(self.in_planes, planes * block.expansion, stride),\n",
    "                                       norm_layer(planes * block.expansion))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_planes, planes, stride, downsample, 1, 64, 1, norm_layer))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(\n",
    "                block(self.in_planes, planes, 1, groups=1, base_width=64, dilation=False, norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(self.bn1(x))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.conv5(x)\n",
    "        x_shape=x.shape\n",
    "        x = self.activation(self.bn5(x)).view(x_shape[0],x_shape[1], -1)\n",
    "        stats = self.attention(x.permute(0, 2, 1).contiguous())\n",
    "        feat = self.fc(stats)\n",
    "\n",
    "        mu = self.fc_mu(feat)\n",
    "\n",
    "        return feat, mu"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T20:37:35.499011800Z",
     "start_time": "2024-04-14T20:37:35.487362900Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T20:37:37.148090900Z",
     "start_time": "2024-04-14T20:37:37.139213300Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "def obtain_asv_error_rates(tar_asv, non_asv, spoof_asv, asv_threshold):\n",
    "\n",
    "    # False alarm and miss rates for ASV\n",
    "    Pfa_asv = sum(non_asv >= asv_threshold) / non_asv.size\n",
    "    Pmiss_asv = sum(tar_asv < asv_threshold) / tar_asv.size\n",
    "\n",
    "    # Rate of rejecting spoofs in ASV\n",
    "    if spoof_asv.size == 0:\n",
    "        Pmiss_spoof_asv = None\n",
    "    else:\n",
    "        Pmiss_spoof_asv = np.sum(spoof_asv < asv_threshold) / spoof_asv.size\n",
    "\n",
    "    return Pfa_asv, Pmiss_asv, Pmiss_spoof_asv\n",
    "\n",
    "\n",
    "def compute_det_curve(target_scores, nontarget_scores):\n",
    "\n",
    "    n_scores = target_scores.size + nontarget_scores.size\n",
    "    all_scores = np.concatenate((target_scores, nontarget_scores))\n",
    "    labels = np.concatenate((np.ones(target_scores.size), np.zeros(nontarget_scores.size)))\n",
    "\n",
    "    # Sort labels based on scores\n",
    "    indices = np.argsort(all_scores, kind='mergesort')\n",
    "    labels = labels[indices]\n",
    "\n",
    "    # Compute false rejection and false acceptance rates\n",
    "    tar_trial_sums = np.cumsum(labels)\n",
    "    nontarget_trial_sums = nontarget_scores.size - (np.arange(1, n_scores + 1) - tar_trial_sums)\n",
    "\n",
    "    frr = np.concatenate((np.atleast_1d(0), tar_trial_sums / target_scores.size))  # false rejection rates\n",
    "    far = np.concatenate((np.atleast_1d(1), nontarget_trial_sums / nontarget_scores.size))  # false acceptance rates\n",
    "    thresholds = np.concatenate((np.atleast_1d(all_scores[indices[0]] - 0.001), all_scores[indices]))  # Thresholds are the sorted scores\n",
    "\n",
    "    return frr, far, thresholds\n",
    "\n",
    "\n",
    "def compute_eer(target_scores, nontarget_scores):\n",
    "    \"\"\" Returns equal error rate (EER) and the corresponding threshold. \"\"\"\n",
    "    frr, far, thresholds = compute_det_curve(target_scores, nontarget_scores)\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    return eer, thresholds[min_index]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-08T22:16:38.668987Z",
     "iopub.execute_input": "2024-04-08T22:16:38.669342Z",
     "iopub.status.idle": "2024-04-08T22:16:38.684338Z",
     "shell.execute_reply.started": "2024-04-08T22:16:38.669312Z",
     "shell.execute_reply": "2024-04-08T22:16:38.683478Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T10:53:05.320832100Z",
     "start_time": "2024-04-15T10:53:05.308156400Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_mfcc_feats(x):\n",
    "    mfcc = librosa.feature.mfcc(y=x, sr=16000, n_mfcc=24)\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    delta2 = librosa.feature.delta(delta)\n",
    "    feats = np.concatenate((mfcc, delta, delta2), axis=0)\n",
    "    return feats"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-08T22:16:38.685379Z",
     "iopub.execute_input": "2024-04-08T22:16:38.685643Z",
     "iopub.status.idle": "2024-04-08T22:16:38.698276Z",
     "shell.execute_reply.started": "2024-04-08T22:16:38.685620Z",
     "shell.execute_reply": "2024-04-08T22:16:38.697439Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T10:53:07.031883300Z",
     "start_time": "2024-04-15T10:53:07.021623500Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_log_spectrum(x):\n",
    "    s = librosa.core.stft(x, n_fft=2048, win_length=2048, hop_length=512)\n",
    "    a = np.abs(s)**2\n",
    "    #melspect = librosa.feature.melspectrogram(S=a)\n",
    "    feat = librosa.power_to_db(a)\n",
    "    return feat"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-08T22:16:38.701103Z",
     "iopub.execute_input": "2024-04-08T22:16:38.701421Z",
     "iopub.status.idle": "2024-04-08T22:16:38.712025Z",
     "shell.execute_reply.started": "2024-04-08T22:16:38.701398Z",
     "shell.execute_reply": "2024-04-08T22:16:38.711182Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T10:53:07.518921100Z",
     "start_time": "2024-04-15T10:53:07.501209100Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "def initParams():\n",
    "    args = {\n",
    "        'num_epochs': 50,\n",
    "        'batch_size': 64,\n",
    "        'lr': 0.0003,\n",
    "        'lr_decay': 0.5,\n",
    "        'interval': 10,\n",
    "        'beta_1': 0.9,\n",
    "        'beta_2': 0.999,\n",
    "        'eps': 1e-8,\n",
    "        'gpu': \"1\",\n",
    "        'num_workers': 0,\n",
    "        'seed': 598,\n",
    "        'add_loss': \"ocsoftmax\",\n",
    "        'weight_loss': 1,\n",
    "        'r_real': 0.9,\n",
    "        'r_fake': 0.2,\n",
    "        'alpha': 20,\n",
    "        'continue_training': False,\n",
    "        'out_fold':'../DL/models/resnet',\n",
    "        'enc_dim':256\n",
    "    }\n",
    "\n",
    "\n",
    "    # Change this to specify GPU\n",
    "\n",
    "    if args['continue_training']:\n",
    "        assert os.path.exists(args['out_fold'])\n",
    "    else:\n",
    "        if not os.path.exists(args['out_fold']):\n",
    "            os.makedirs(args['out_fold'])\n",
    "        else:\n",
    "            shutil.rmtree(args['out_fold'])\n",
    "            os.mkdir(args['out_fold'])\n",
    "\n",
    "        if not os.path.exists(os.path.join(args['out_fold'], 'checkpoint')):\n",
    "            os.makedirs(os.path.join(args['out_fold'], 'checkpoint'))\n",
    "        else:\n",
    "            shutil.rmtree(os.path.join(args['out_fold'], 'checkpoint'))\n",
    "            os.mkdir(os.path.join(args['out_fold'], 'checkpoint'))\n",
    "\n",
    "\n",
    "        with open(os.path.join(args['out_fold'], 'train_loss.log'), 'w') as file:\n",
    "            file.write(\"Start recording training loss ...\\n\")\n",
    "        with open(os.path.join(args['out_fold'], 'dev_loss.log'), 'w') as file:\n",
    "            file.write(\"Start recording validation loss ...\\n\")\n",
    "\n",
    "    args['cuda'] = torch.cuda.is_available()  \n",
    "    print(args['cuda'])\n",
    "    print(torch.cuda.current_device())\n",
    "    args['device'] = torch.device(\"cuda\" if args['cuda'] else \"cpu\")\n",
    "    \n",
    "    return args\n",
    "\n",
    "\n",
    "def adjust_learning_rate(args, optimizer, epoch_num):\n",
    "    lr = args['lr'] * (args['lr_decay'] ** (epoch_num // args['interval']))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def train(args,transforms,dev_set,train_set):\n",
    "    lfcc_model = ResNet(3, args['enc_dim'], resnet_type='18', nclasses=2).to(args['device'])\n",
    "    if args['continue_training']:\n",
    "        lfcc_model = torch.load(os.path.join(args['out_fold'], 'anti-spoofing_lfcc_model.pt')).to(args['device'])\n",
    "\n",
    "    lfcc_optimizer = torch.optim.Adam(lfcc_model.parameters(), lr=args['lr'],\n",
    "                                      betas=(args['beta_1'], args['beta_2']), eps=args['eps'], weight_decay=0.0005)\n",
    " \n",
    "    \n",
    "    \n",
    "    trainDataLoader = DataLoader(train_set, batch_size=args['batch_size'], shuffle=True)\n",
    " \n",
    "    valDataLoader = DataLoader(dev_set, batch_size=args['batch_size'], shuffle=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if args['add_loss'] == \"amsoftmax\":\n",
    "        amsoftmax_loss = AMSoftmax(2, args['enc_dim'], s=args['alpha'], m=args['r_real']).to(args['device'])\n",
    "        amsoftmax_loss.train()\n",
    "        amsoftmax_optimzer = torch.optim.SGD(amsoftmax_loss.parameters(), lr=0.01)\n",
    "\n",
    "    if args['add_loss'] == \"ocsoftmax\":\n",
    "        ocsoftmax = OCSoftmax(args['enc_dim'], r_real=args['r_real'], r_fake=args['r_fake'], alpha=args['alpha']).to(args['device'])\n",
    "        ocsoftmax.train()\n",
    "        ocsoftmax_optimzer = torch.optim.SGD(ocsoftmax.parameters(), lr=args['lr'])\n",
    "\n",
    "    early_stop_cnt = 0\n",
    "    prev_eer = 1e8\n",
    "\n",
    "    monitor_loss = args['add_loss']\n",
    "\n",
    "    for epoch_num in range(args['num_epochs']):\n",
    "        lfcc_model.train()\n",
    "        print(lfcc_model.device)\n",
    "        trainlossDict = defaultdict(list)\n",
    "        devlossDict = defaultdict(list)\n",
    "        adjust_learning_rate(args, lfcc_optimizer, epoch_num)\n",
    "        if args['add_loss'] == \"ocsoftmax\":\n",
    "            adjust_learning_rate(args, ocsoftmax_optimzer, epoch_num)\n",
    "        elif args['add_loss'] == \"amsoftmax\":\n",
    "            adjust_learning_rate(args, amsoftmax_optimzer, epoch_num)\n",
    "        print('\\nEpoch: %d ' % (epoch_num + 1))\n",
    "        for i, (lfcc, labels) in enumerate(tqdm(trainDataLoader)):\n",
    "            lfcc = lfcc.unsqueeze(1).float().to(args['device'])\n",
    "            labels = labels.type(torch.LongTensor)  \n",
    "            labels = labels.to(args['device'])\n",
    "            feats, lfcc_outputs = lfcc_model(lfcc)\n",
    "            lfcc_loss = criterion(lfcc_outputs, labels)\n",
    "\n",
    "            if args['add_loss'] == \"softmax\":\n",
    "                lfcc_optimizer.zero_grad()\n",
    "                trainlossDict[args['add_loss']].append(lfcc_loss.item())\n",
    "                lfcc_loss.backward()\n",
    "                lfcc_optimizer.step()\n",
    "\n",
    "            if args['add_loss'] == \"ocsoftmax\":\n",
    "                ocsoftmaxloss, _ = ocsoftmax(feats, labels)\n",
    "                lfcc_loss = ocsoftmaxloss * args['weight_loss']\n",
    "                lfcc_optimizer.zero_grad()\n",
    "                ocsoftmax_optimzer.zero_grad()\n",
    "                trainlossDict[args['add_loss']].append(ocsoftmaxloss.item())\n",
    "                lfcc_loss.backward()\n",
    "                lfcc_optimizer.step()\n",
    "                ocsoftmax_optimzer.step()\n",
    "\n",
    "            if args['add_loss'] == \"amsoftmax\":\n",
    "                outputs, moutputs = amsoftmax_loss(feats, labels)\n",
    "                lfcc_loss = criterion(moutputs, labels)\n",
    "                trainlossDict[args['add_loss']].append(lfcc_loss.item())\n",
    "                lfcc_optimizer.zero_grad()\n",
    "                amsoftmax_optimzer.zero_grad()\n",
    "                lfcc_loss.backward()\n",
    "                lfcc_optimizer.step()\n",
    "                amsoftmax_optimzer.step()\n",
    "\n",
    "            # with open(os.path.join(args['out_fold'], \"train_loss.log\"), \"a\") as log:\n",
    "            #     log.write(str(epoch_num) + \"\\t\" + str(i) + \"\\t\" +\n",
    "            #               str(np.nanmean(trainlossDict[monitor_loss])) + \"\\n\")\n",
    "\n",
    "        lfcc_model.eval()\n",
    "        with torch.no_grad():\n",
    "            idx_loader, score_loader = [], []\n",
    "            for i, (lfcc, labels) in enumerate(valDataLoader):\n",
    "                lfcc = lfcc.unsqueeze(1).float().to(args['device'])\n",
    "                labels = labels.type(torch.LongTensor) \n",
    "                labels = labels.to(args['device'])\n",
    "                feats, lfcc_outputs = lfcc_model(lfcc)\n",
    "\n",
    "                lfcc_loss = criterion(lfcc_outputs.float(), labels)\n",
    "                score = F.softmax(lfcc_outputs, dim=1)[:, 0]\n",
    "\n",
    "                if args['add_loss'] == \"softmax\":\n",
    "                    devlossDict[\"softmax\"].append(lfcc_loss.item())\n",
    "                elif args['add_loss'] == \"amsoftmax\":\n",
    "                    outputs, moutputs = amsoftmax_loss(feats, labels)\n",
    "                    lfcc_loss = criterion(moutputs, labels)\n",
    "                    score = F.softmax(outputs, dim=1)[:, 0]\n",
    "                    devlossDict[args['add_loss']].append(lfcc_loss.item())\n",
    "                elif args['add_loss'] == \"ocsoftmax\":\n",
    "                    ocsoftmaxloss, score = ocsoftmax(feats, labels)\n",
    "                    devlossDict[args['add_loss']].append(ocsoftmaxloss.item())\n",
    "                idx_loader.append(labels)\n",
    "                score_loader.append(score)\n",
    "\n",
    "            scores = torch.cat(score_loader, 0).data.cpu().numpy()\n",
    "            labels = torch.cat(idx_loader, 0).data.cpu().numpy()\n",
    "            val_eer = compute_eer(scores[labels == 0], scores[labels == 1])[0]\n",
    "\n",
    "            with open(os.path.join(args['out_fold'], \"dev_loss.log\"), \"a\") as log:\n",
    "                log.write(\n",
    "                    str(epoch_num) + \"\\t\" + str(np.nanmean(devlossDict[monitor_loss])) + \"\\t\" + str(val_eer) + \"\\n\")\n",
    "            print(\"Val EER: {}\".format(val_eer))\n",
    "\n",
    "        torch.save(lfcc_model, os.path.join(args['out_fold'], 'checkpoint',\n",
    "                                            'anti-spoofing_lfcc_model_%d.pt' % (epoch_num + 1)))\n",
    "        if args['add_loss'] == \"ocsoftmax\":\n",
    "            loss_model = ocsoftmax\n",
    "            torch.save(loss_model, os.path.join(args['out_fold'], 'checkpoint',\n",
    "                                                'anti-spoofing_loss_model_%d.pt' % (epoch_num + 1)))\n",
    "        elif args['add_loss'] == \"amsoftmax\":\n",
    "            loss_model = amsoftmax_loss\n",
    "            torch.save(loss_model, os.path.join(args['out_fold'], 'checkpoint',\n",
    "                                                'anti-spoofing_loss_model_%d.pt' % (epoch_num + 1)))\n",
    "        else:\n",
    "            loss_model = None\n",
    "\n",
    "        if val_eer < prev_eer:\n",
    "            torch.save(lfcc_model, os.path.join(args['out_fold'], 'anti-spoofing_lfcc_model.pt'))\n",
    "            if args['add_loss'] == \"ocsoftmax\":\n",
    "                loss_model = ocsoftmax\n",
    "                torch.save(loss_model, os.path.join(args['out_fold'], 'anti-spoofing_loss_model.pt'))\n",
    "            elif args['add_loss'] == \"amsoftmax\":\n",
    "                loss_model = amsoftmax_loss\n",
    "                torch.save(loss_model, os.path.join(args['out_fold'], 'anti-spoofing_loss_model.pt'))\n",
    "            else:\n",
    "                loss_model = None\n",
    "            prev_eer = val_eer\n",
    "            early_stop_cnt = 0\n",
    "        else:\n",
    "            early_stop_cnt += 1\n",
    "\n",
    "        if early_stop_cnt == 100:\n",
    "            with open(os.path.join(args['out_fold'], 'args.json'), 'a') as res_file:\n",
    "                res_file.write('\\nTrained Epochs: %d\\n' % (epoch_num - 19))\n",
    "            break\n",
    "\n",
    "    return lfcc_model, loss_model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-08T22:16:38.713261Z",
     "iopub.execute_input": "2024-04-08T22:16:38.713552Z",
     "iopub.status.idle": "2024-04-08T22:16:38.756410Z",
     "shell.execute_reply.started": "2024-04-08T22:16:38.713527Z",
     "shell.execute_reply": "2024-04-08T22:16:38.755537Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T10:53:21.388977900Z",
     "start_time": "2024-04-15T10:53:21.361945700Z"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "feature_fn = compute_mfcc_feats\n",
    "transforms = transforms.Compose([\n",
    "lambda x: pad(x),\n",
    "lambda x: librosa.util.normalize(x),\n",
    "lambda x: feature_fn(x),\n",
    "lambda x: Tensor(x)\n",
    "])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-08T22:16:38.757499Z",
     "iopub.execute_input": "2024-04-08T22:16:38.757758Z",
     "iopub.status.idle": "2024-04-08T22:16:38.770479Z",
     "shell.execute_reply.started": "2024-04-08T22:16:38.757736Z",
     "shell.execute_reply": "2024-04-08T22:16:38.769580Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T10:53:13.053706100Z",
     "start_time": "2024-04-15T10:53:13.041543300Z"
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "args = initParams()\n",
    "model=train(args,transforms,dev_set,train_set)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-08T22:16:42.980481Z",
     "iopub.execute_input": "2024-04-08T22:16:42.982690Z"
    },
    "trusted": true,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-15T10:53:25.824950700Z"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PFA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 387/844 [03:36<03:49,  1.99it/s]"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_model_ocsoftmax(feat_model_path, loss_model_path, part, add_loss, device):\n",
    "    dirname = os.path.dirname\n",
    "    basename = os.path.splitext(os.path.basename(feat_model_path))[0]\n",
    "    model_feats=[]\n",
    "    model_labels=[]\n",
    "    count=0\n",
    "    prev_label=None\n",
    "    if \"checkpoint\" in dirname(feat_model_path):\n",
    "        dir_path = dirname(dirname(feat_model_path))\n",
    "    else:\n",
    "        dir_path = dirname(feat_model_path)\n",
    "    model=torch.load(feat_model_path,map_location='cuda').to(device)\n",
    "    loss_model=torch.load(loss_model_path,map_location='cuda').to(device)\n",
    "    is_eval = (part == 'eval')\n",
    "    \n",
    "    test_set = SoundFeatureDataset('../Data/PA','../DL/mfcc',is_logical=False, is_train=True)\n",
    "    testDataLoader = DataLoader(test_set, batch_size=10, shuffle=False)\n",
    "    model.eval()\n",
    "\n",
    "    with open(os.path.join('../DL/models/resnet/', 'checkpoint_cm_score.txt'), 'w') as cm_score_file:\n",
    "        for i, (lfcc, labels,filenames) in enumerate(tqdm(testDataLoader)):\n",
    "            lfcc = lfcc.unsqueeze(1).float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            feats, lfcc_outputs = model(lfcc)\n",
    "            \n",
    "            score = F.softmax(lfcc_outputs)[:, 0]\n",
    "\n",
    "            if add_loss == \"ocsoftmax\":\n",
    "                ang_isoloss, score = loss_model(feats, labels)\n",
    "            elif add_loss == \"amsoftmax\":\n",
    "                outputs, moutputs = loss_model(feats, labels)\n",
    "                score = F.softmax(outputs, dim=1)[:, 0]\n",
    "\n",
    "            for j in range(labels.size(0)):\n",
    "                if count<=300:\n",
    "                    if prev_label!=labels[j]:\n",
    "                        model_feats.append(feats[j].detach().cpu().numpy())\n",
    "                        model_labels.append(\"spoof\" if labels[j].data.cpu().numpy() else \"bonafide\")\n",
    "                        prev_label=labels[j]\n",
    "                        count=count+1\n",
    "                cm_score_file.write(\n",
    "                    '%s %s %s\\n' % (filenames[j],\"spoof\" if labels[j].data.cpu().numpy() else \"bonafide\",score[j].item()))\n",
    "    \n",
    "    output_file = os.path.join('../DL/models/resnet/', 'feats_' + 'mfcc_resnet' + \".tsv\")\n",
    "    meta_file = os.path.join('../DL/models/resnet/', 'meta_' + 'mfcc_resnet' + \".tsv\")\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        for feat in model_feats:\n",
    "            sample_str = '\\t'.join([str(e) for e in feat])\n",
    "            f.write(f\"{sample_str}\\n\")\n",
    "    \n",
    "    with open(meta_file, 'w') as meta:\n",
    "        for label in model_labels:\n",
    "            meta.write(f\"{label}\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "def test_ocsoftmax(model_dir, model, loss_model, add_loss, device):\n",
    "    model_path = os.path.join(model_dir, model)\n",
    "    loss_model_path = os.path.join(model_dir, loss_model)\n",
    "    test_model_ocsoftmax(model_path, loss_model_path, \"eval\", add_loss, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T14:55:44.135677500Z",
     "start_time": "2024-04-14T14:55:44.126296100Z"
    }
   },
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/models\\\\anti-spoofing_mfcc_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m loss_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124manti-spoofing_loss_mfcc_model .pt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      5\u001B[0m add_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mocsoftmax\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 6\u001B[0m \u001B[43mtest_ocsoftmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[38], line 61\u001B[0m, in \u001B[0;36mtest_ocsoftmax\u001B[1;34m(model_dir, model, loss_model, add_loss, device)\u001B[0m\n\u001B[0;32m     59\u001B[0m model_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(model_dir, model)\n\u001B[0;32m     60\u001B[0m loss_model_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(model_dir, loss_model)\n\u001B[1;32m---> 61\u001B[0m \u001B[43mtest_model_ocsoftmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_model_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meval\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[38], line 12\u001B[0m, in \u001B[0;36mtest_model_ocsoftmax\u001B[1;34m(feat_model_path, loss_model_path, part, add_loss, device)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     11\u001B[0m     dir_path \u001B[38;5;241m=\u001B[39m dirname(feat_model_path)\n\u001B[1;32m---> 12\u001B[0m model\u001B[38;5;241m=\u001B[39m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeat_model_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     13\u001B[0m loss_model\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mload(loss_model_path,map_location\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     14\u001B[0m is_eval \u001B[38;5;241m=\u001B[39m (part \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meval\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mD:\\PFA\\venv\\Lib\\site-packages\\torch\\serialization.py:998\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m    995\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    996\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 998\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    999\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m   1000\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m   1001\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m   1002\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m   1003\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32mD:\\PFA\\venv\\Lib\\site-packages\\torch\\serialization.py:445\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    444\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 445\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    447\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32mD:\\PFA\\venv\\Lib\\site-packages\\torch\\serialization.py:426\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    425\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 426\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../Data/models\\\\anti-spoofing_mfcc_model.pt'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir = '../Data/models/resnet/v1'\n",
    "model = 'anti-spoofing_mfcc_model.pt'\n",
    "loss_model = 'anti-spoofing_loss_mfcc_model .pt'\n",
    "add_loss = 'ocsoftmax'\n",
    "test_ocsoftmax(model_dir, model, loss_model, add_loss,device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-14T14:56:02.788086500Z",
     "start_time": "2024-04-14T14:56:02.734922300Z"
    }
   },
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from embedding import *\n",
    "import numpy as np\n",
    "def compute_det_curve(target_scores, nontarget_scores):\n",
    "\n",
    "    n_scores = target_scores.size + nontarget_scores.size\n",
    "    all_scores = np.concatenate((target_scores, nontarget_scores))\n",
    "    labels = np.concatenate((np.ones(target_scores.size), np.zeros(nontarget_scores.size)))\n",
    "\n",
    "    # Sort labels based on scores\n",
    "    indices = np.argsort(all_scores, kind='mergesort')\n",
    "    labels = labels[indices]\n",
    "\n",
    "    # Compute false rejection and false acceptance rates\n",
    "    tar_trial_sums = np.cumsum(labels)\n",
    "    nontarget_trial_sums = nontarget_scores.size - (np.arange(1, n_scores + 1) - tar_trial_sums)\n",
    "\n",
    "    frr = np.concatenate((np.atleast_1d(0), tar_trial_sums / target_scores.size))  # false rejection rates\n",
    "    far = np.concatenate((np.atleast_1d(1), nontarget_trial_sums / nontarget_scores.size))  # false acceptance rates\n",
    "    thresholds = np.concatenate((np.atleast_1d(all_scores[indices[0]] - 0.001), all_scores[indices]))  # Thresholds are the sorted scores\n",
    "\n",
    "    return frr, far, thresholds\n",
    "\n",
    "\n",
    "def compute_eer(target_scores, nontarget_scores):\n",
    "    \"\"\" Returns equal error rate (EER) and the corresponding threshold. \"\"\"\n",
    "    frr, far, thresholds = compute_det_curve(target_scores, nontarget_scores)\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    return eer*100, thresholds[min_index]\n",
    "\n",
    "\n",
    "scores_fname='res/lfcc_renset18_cm_score.txt'\n",
    "target_scores=[]\n",
    "nontarget_scores=[]\n",
    "scores=[]\n",
    "y_true=[]\n",
    "lines_score=open(scores_fname).readlines()\n",
    "for line_score in lines_score:\n",
    "    score = float(line_score.strip().split(' ')[2])\n",
    "    scores.append(score)\n",
    "    meta_1 = line_score.strip().split(' ')[0]\n",
    "    y = int(line_score.strip().split(' ')[1] == 'spoof')\n",
    "    y_true.append(y)\n",
    "    if(y==0):\n",
    "        target_scores.append(score)\n",
    "    else:\n",
    "        nontarget_scores.append(score)\n",
    "eer_roc=compute_eer(np.array(target_scores), np.array(nontarget_scores))\n",
    "print(\"eer(%):\",eer_roc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# feature_fn = extract_lfcc\n",
    "# transforms = transforms.Compose([\n",
    "# lambda x: pad(x),\n",
    "# lambda x: librosa.util.normalize(x),\n",
    "# lambda x: feature_fn(x),\n",
    "# lambda x: Tensor(x)\n",
    "# ])"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T12:16:52.644641600Z",
     "start_time": "2024-04-14T12:16:52.642197700Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# def test_model_ocsoftmax(feat_model_path, loss_model_path, part, add_loss, device):\n",
    "#     dirname = os.path.dirname\n",
    "#     basename = os.path.splitext(os.path.basename(feat_model_path))[0]\n",
    "#     if \"checkpoint\" in dirname(feat_model_path):\n",
    "#         dir_path = dirname(dirname(feat_model_path))\n",
    "#     else:\n",
    "#         dir_path = dirname(feat_model_path)\n",
    "#     print(feat_model_path)\n",
    "#     print(loss_model_path)\n",
    "#     model=torch.load(feat_model_path,map_location='cuda').to(device)\n",
    "#     loss_model=torch.load(loss_model_path,map_location='cuda').to(device)\n",
    "#     is_eval = (part == 'eval')\n",
    "#     test_set = ASVDataset('/kaggle/input/asvpoof-2019-dataset/LA/LA', is_train=False, is_eval=True, is_logical=True, transform=transforms,\n",
    "#                           feature_name='lfcc',random_sample=30000)\n",
    "#     testDataLoader = DataLoader(test_set, batch_size=10, shuffle=False)\n",
    "#     model.eval()\n",
    "\n",
    "#     with open(os.path.join('/kaggle/working/', 'checkpoint_cm_score.txt'), 'w') as cm_score_file:\n",
    "#         for i, (lfcc, labels,filenames) in enumerate(tqdm(testDataLoader)):\n",
    "#             lfcc = lfcc.unsqueeze(1).float().to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             feats, lfcc_outputs = model(lfcc)\n",
    "\n",
    "#             score = F.softmax(lfcc_outputs)[:, 0]\n",
    "\n",
    "#             if add_loss == \"ocsoftmax\":\n",
    "#                 ang_isoloss, score = loss_model(feats, labels)\n",
    "#             elif add_loss == \"amsoftmax\":\n",
    "#                 outputs, moutputs = loss_model(feats, labels)\n",
    "#                 score = F.softmax(outputs, dim=1)[:, 0]\n",
    "\n",
    "#             for j in range(labels.size(0)):\n",
    "#                 cm_score_file.write(\n",
    "#                     '%s %s %s\\n' % (filenames[j],\"spoof\" if labels[j].data.cpu().numpy() else \"bonafide\",score[j].item()))\n",
    "\n",
    "\n",
    "# def test_ocsoftmax(model_dir, model, loss_model, add_loss, device):\n",
    "#     model_path = os.path.join(model_dir, model)\n",
    "#     loss_model_path = os.path.join(model_dir, loss_model)\n",
    "#     test_model_ocsoftmax(model_path, loss_model_path, \"eval\", add_loss, device)\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_dir = '/kaggle/input/resnt_ossoftamx/pytorch/resnet_models/1'\n",
    "# model = 'anti-spoofing_lfcc_model.pt'\n",
    "# loss_model = 'anti-spoofing_loss_model.pt'\n",
    "# add_loss = 'ocsoftmax'\n",
    "# test_ocsoftmax(model_dir, model, loss_model, add_loss,device)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T12:16:52.653433400Z",
     "start_time": "2024-04-14T12:16:52.645683900Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
